{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LCkjFG5C2xO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,Bidirectional, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy\n",
        "\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m0pua21DBix",
        "outputId": "7dcadcc7-0e3a-4bb4-a0f1-d4c442b96f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWBe5ly2DLoy",
        "outputId": "dfeac87d-ce5c-4c1e-cebf-fe0cc4784bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
            "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
            "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
            "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
            "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             0        0       0       0              0  \n",
            "1             0        0       0       0              0  \n",
            "2             0        0       0       0              0  \n",
            "3             0        0       0       0              0  \n",
            "4             0        0       0       0              0  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(r\"/content/gdrive/MyDrive/database/training/train.csv\")\n",
        "\n",
        "\n",
        "# lets check the glimpse of first five rows of train dataset\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITfUF4NTDPpe",
        "outputId": "2cd1c0bc-669c-4c01-a171-54591b7e8336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(159571, 8)\n",
            "5000\n"
          ]
        }
      ],
      "source": [
        "# shape of train dataframe\n",
        "print(df.shape)\n",
        "\n",
        "# Separate Independent and Dependent Variables\n",
        "X = df['comment_text']\n",
        "y = df.loc[:, 'toxic':].values\n",
        "\n",
        "\n",
        "# lets check the comment maximum length \n",
        "print(df['comment_text'].str.len().max())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ma4dj7JDcnj"
      },
      "outputs": [],
      "source": [
        "MAX_FEATURES = 200000 # number of words in the vocab\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Text Vectorization is the process of converting text into a numerical representation.\n",
        "# It transforms text into a more suitable form so that ML or DL algorithms can perform better.\n",
        "vectorizer = TextVectorization(max_tokens=MAX_FEATURES, \n",
        "                               output_sequence_length=1800,  # Maximum comment (text) size in words\n",
        "                               output_mode='int')\n",
        "\n",
        "\n",
        "\n",
        "vectorizer.adapt(X.values)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# apply vectorizer on X\n",
        "vectorized_text = vectorizer(X.values)\n",
        "\n",
        "# prepare tensorflow dataset\n",
        "# it converts the data stored in Pandas Data Frame into data stored in TensorFlow Data Set.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y))\n",
        "# cache keeps the images in memory after they're loaded off disk during the first epoch.\n",
        "dataset = dataset.cache()\n",
        "# For true randomness, we set the shuffle buffer to the full dataset size.\n",
        "dataset = dataset.shuffle(160000)\n",
        "# Batch after shuffling to get unique batches at each epoch\n",
        "dataset = dataset.batch(16)\n",
        "# prefetch overlaps data preprocessing and model execution while training.\n",
        "dataset = dataset.prefetch(8) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwErmFu2Dt_f"
      },
      "outputs": [],
      "source": [
        "# train(70%), validation(20%) and test(10%) split\n",
        "train = dataset.take(int(len(dataset)*.7))\n",
        "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
        "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1OJVdnQDwV_"
      },
      "outputs": [],
      "source": [
        "# Formation of CNN model\n",
        "# Sequential layer\n",
        "model = Sequential()\n",
        "# Create the embedding layer \n",
        "model.add(Embedding(MAX_FEATURES+1, 32))\n",
        "# Bidirectional LSTM Layer\n",
        "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
        "# Feature extractor Fully connected layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# Final layer/ output layer \n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeVuoCL8D4wW",
        "outputId": "7f0edac8-9d13-4355-9ed1-8df1e6799702"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-88e21cc9eff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compile the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BinaryCrossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Compile the Model\n",
        "model.compile(loss='BinaryCrossentropy', optimizer='Adam')\n",
        "\n",
        "\n",
        "\n",
        "# model summary\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "# fit model for only 9 epochs\n",
        "history = model.fit(train, epochs=20, validation_data=val)\n",
        "print(history)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# save the model for future use\n",
        "model.save('toxic_comments_model.h5')\n",
        "\n",
        "\n",
        "# save the model for future use\n",
        "model.save('toxic_comments_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "jBxr_tqlw3Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "model = tf.keras.models.load_model('toxic_comments_model.h5')\n",
        "\n",
        "# use text vectorization on raw data to test the model\n",
        "input_text = vectorizer(np.expand_dims('I am going to hit you.',0))\n",
        "\n",
        "# model prediction on raw text\n",
        "res = model.predict(input_text)\n",
        "print(res)\n",
        "\n",
        "\n",
        "\n",
        "# lets check the precision, recall and accuracy on test dataset \n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = CategoricalAccuracy()\n",
        "\n",
        "for batch in test.as_numpy_iterator(): \n",
        "    # Unpack the batch \n",
        "    X_true, y_true = batch\n",
        "    # Make a prediction \n",
        "    yhat = model.predict(X_true)\n",
        "    \n",
        "    # Flatten the true values\n",
        "    y_true = y_true.flatten()\n",
        "    # Flatten the predictions\n",
        "    yhat = yhat.flatten()\n",
        "    \n",
        "    pre.update_state(y_true, yhat)\n",
        "    re.update_state(y_true, yhat)\n",
        "    acc.update_state(y_true, yhat)\n",
        "\n",
        "print(f'Precision: {pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')"
      ],
      "metadata": {
        "id": "q1QxtoUMw62W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_comment(comment):\n",
        "    vectorized_comment = vectorizer([comment])\n",
        "    results = model.predict(vectorized_comment)\n",
        "    \n",
        "    text = ''\n",
        "    for idx, col in enumerate(df.columns[2:]):\n",
        "        text += '{}: {}\\n'.format(col, results[0][idx]>0.5)\n",
        "    \n",
        "    return text\n",
        "\n",
        "# print(\"statement 1\")\n",
        "# print(score_comment('you are stupid.'))\n",
        "# print(\"statement 2\")\n",
        "# print(score_comment('This is shit.'))\n",
        "# print(\"statement 3\")\n",
        "# print(score_comment('I am going to kill You'))\n",
        "# print(\"statement 4\")\n",
        "# print(score_comment('I am a student'))\n",
        "\n",
        "\n",
        "\n",
        "# save the labels for further use in deployment\n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "pickle.dump(labels,open('labels.pkl','wb'))"
      ],
      "metadata": {
        "id": "MPbAD7Qgw8uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGuwnFsEw-bd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}